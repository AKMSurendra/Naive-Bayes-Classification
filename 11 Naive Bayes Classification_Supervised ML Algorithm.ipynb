{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is a classifier?\n",
    "---------------------\n",
    "A classifier is a machine learning model that is used to discriminate different objects based on certain features.\n",
    "\n",
    "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. The crux of the classifier is based on the Bayes theorem (explained below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bayes theorem](datasets_n_images/images/Bayestheorem_formulae.png 'Bayestheorem_formulae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Points:\n",
    "--\n",
    ">__1. NB gives better result in multi class problems.__\n",
    "\n",
    "And\n",
    "\n",
    ">__2. NB gives better results when there is least colinearity among independent features.__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "What is Naive Bayes algorithm?\n",
    "-------------------------------\n",
    "It is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. \n",
    "\n",
    "For example, a fruit may be considered to be an apple if it is red, round, and about 3 inches in diameter. Even if these features depend on each other or upon the existence of the other features, all of these properties independently contribute to the probability that this fruit is an apple and that is why it is known as ‘Naive’.\n",
    "\n",
    "Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n",
    "\n",
    "Bayes theorem provides a way of calculating posterior or conditional probability P(c|x) from P(c), P(x) and P(x|c). Look at the equation below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NaiveBayesFormalae](datasets_n_images/images/naive_bayes_formulae.png 'naive_bayes_formulae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Naive Bayes algorithm works?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let’s understand it using an example. Below I have a training data set of weather and corresponding target variable ‘Play’ (suggesting possibilities of playing). Now, we need to classify whether players will play or not based on weather condition. Let’s follow the below steps to perform it.\n",
    "\n",
    "Step 1: Convert the data set into a frequency table\n",
    "\n",
    "Step 2: Create Likelihood table by finding the probabilities like Overcast probability = 0.29 and probability of playing is 0.64."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['Example Tables'](datasets_n_images/images/NaiveBayes_example_tables.png 'NaiveBayes_example_tables')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Step 3: Now, use Naive Bayesian equation to calculate the posterior probability for each class. The class with the highest posterior probability is the outcome of prediction."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Problem: Players will play if weather is sunny. Is this statement is correct?\n",
    "-----------------------------------------------------------------------------\n",
    "We can solve it using above discussed method of posterior probability.\n",
    "\n",
    "P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)\n",
    "\n",
    "Here we have P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P(Yes)= 9/14 = 0.64\n",
    "\n",
    "Now, P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60, which has higher probability.\n",
    "\n",
    "Naive Bayes uses a similar method to predict the probability of different class based on various attributes. \n",
    "\n",
    "This algorithm is mostly used in text classification and with problems having multiple classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A basic model using Naive Bayes in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3  7]\n",
      " [ 1  5]\n",
      " [ 1  2]\n",
      " [-2  0]\n",
      " [ 2  3]\n",
      " [-4  0]\n",
      " [-1  1]\n",
      " [ 1  1]\n",
      " [-2  2]\n",
      " [ 2  7]\n",
      " [-4  1]\n",
      " [-2  7]] \n",
      "\n",
      " (12, 2)\n",
      "\n",
      " [3 3 3 3 4 3 3 4 3 4 4 4] \n",
      "\n",
      " (12,)\n"
     ]
    }
   ],
   "source": [
    "# scikit learn (python library) will help here to build a Naive Bayes model \n",
    "# in Python.  There are three types of Naive Bayes model \n",
    "# under scikit learn library:\n",
    "\n",
    "# 1> Gaussian : Used in classification and assumes that features \n",
    "#               follow a normal distribution.\n",
    "# 2> Multinomial (http://mathworld.wolfram.com/MultinomialDistribution.html)\n",
    "# 3> Bernoulli (http://mathworld.wolfram.com/BernoulliDistribution.html)\n",
    "\n",
    "# Below is the example of Gaussian model.\n",
    "\n",
    "# Import Library of Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# assigning predictor and target variables\n",
    "x = np.array([[-3,7],[1,5], [1,2], [-2,0], [2,3], \n",
    "             [-4,0], [-1,1], [1,1], [-2,2], [2,7], [-4,1], [-2,7]])\n",
    "print(x,\"\\n\\n\",x.shape)\n",
    "Y = np.array([3, 3, 3, 3, 4, 3, 3, 4, 3, 4, 4, 4])\n",
    "print(\"\\n\",Y,\"\\n\\n\",Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4]\n"
     ]
    }
   ],
   "source": [
    "# type your code. Comments shouldguide you.\n",
    "# Create a Gaussian Classifier\n",
    "model=GaussianNB()\n",
    "\n",
    "# Train the model using the training sets \n",
    "model.fit(x,Y)\n",
    "\n",
    "# Predict Output \n",
    "predicted=model.predict([[1,2],[3,4]])\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where is Naive Bayes Classifier used ?\n",
    "\n",
    ">1 Real time Prediction \n",
    "\n",
    ">2 Text classification / Spam Filtering \n",
    "\n",
    ">3 Recommendation System "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the Pros and Cons of Naive Bayes?\n",
    "\n",
    "Pros:\n",
    "-------\n",
    "1> It is easy and fast to predict class of test data set. It also performs well in multi class prediction. \n",
    "\n",
    "2> When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
    "\n",
    "3> It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption). \n",
    "\n",
    "Cons:\n",
    "--------\n",
    "1> If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction(thus it becomes an outlier. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing(normalizing) techniques is called Laplace estimation.\n",
    "\n",
    "2> On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_probability are not to be taken too seriously.\n",
    "\n",
    "3> Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets analyze the titanic dataset to predict whether a person Survived or not.\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train and test dataset\n",
    "train_data = pd.read_csv(\"./datasets_n_images/datasets_module_4/train-data.csv\")\n",
    "test_data = pd.read_csv('./datasets_n_images/datasets_module_4/test-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Survived        Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
      "0         0  28.500000   7.2292         0         0         1           0   \n",
      "1         1  27.000000  10.5000         0         1         0           1   \n",
      "2         1  29.699118  16.1000         0         0         1           1   \n",
      "3         0  29.699118   0.0000         1         0         0           0   \n",
      "4         0  17.000000   8.6625         0         0         1           0   \n",
      "\n",
      "   Sex_male  SibSp_0  SibSp_1  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
      "0         1        1        0  ...        1        0        0        0   \n",
      "1         0        1        0  ...        1        0        0        0   \n",
      "2         0        0        1  ...        1        0        0        0   \n",
      "3         1        1        0  ...        1        0        0        0   \n",
      "4         1        1        0  ...        1        0        0        0   \n",
      "\n",
      "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0        0        0        0           1           0           0  \n",
      "1        0        0        0           0           0           1  \n",
      "2        0        0        0           0           0           1  \n",
      "3        0        0        0           0           0           1  \n",
      "4        0        0        0           0           0           1  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "------------------------\n",
      "   Survived   Age      Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  \\\n",
      "0         0  35.0    7.1250         0         0         1           0   \n",
      "1         0  20.0    7.0500         0         0         1           0   \n",
      "2         0  26.0    7.8958         0         0         1           0   \n",
      "3         1  58.0  146.5208         1         0         0           1   \n",
      "4         1  35.0   83.4750         1         0         0           1   \n",
      "\n",
      "   Sex_male  SibSp_0  SibSp_1  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
      "0         1        1        0  ...        1        0        0        0   \n",
      "1         1        1        0  ...        1        0        0        0   \n",
      "2         1        1        0  ...        1        0        0        0   \n",
      "3         0        1        0  ...        1        0        0        0   \n",
      "4         0        0        1  ...        1        0        0        0   \n",
      "\n",
      "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0        0        0        0           0           0           1  \n",
      "1        0        0        0           0           0           1  \n",
      "2        0        0        0           0           0           1  \n",
      "3        0        0        0           1           0           0  \n",
      "4        0        0        0           0           0           1  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_data.head())\n",
    "print(\"------------------------\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training data : (712, 25)\n",
      "Shape of testing data : (179, 25)\n"
     ]
    }
   ],
   "source": [
    "# shape of the dataset\n",
    "print('Shape of training data :',train_data.shape)\n",
    "print('Shape of testing data :',test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we need to predict the missing target variable in the test data\n",
    "# target variable - Survived\n",
    "\n",
    "# seperate the independent and target variable on training data\n",
    "train_x = train_data.drop(columns=['Survived'],axis=1)\n",
    "train_y = train_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping 'Survived', train_x=\n",
      ":          Age     Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  \\\n",
      "0  28.500000   7.2292         0         0         1           0         1   \n",
      "1  27.000000  10.5000         0         1         0           1         0   \n",
      "2  29.699118  16.1000         0         0         1           1         0   \n",
      "3  29.699118   0.0000         1         0         0           0         1   \n",
      "4  17.000000   8.6625         0         0         1           0         1   \n",
      "\n",
      "   SibSp_0  SibSp_1  SibSp_2  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
      "0        1        0        0  ...        1        0        0        0   \n",
      "1        1        0        0  ...        1        0        0        0   \n",
      "2        0        1        0  ...        1        0        0        0   \n",
      "3        1        0        0  ...        1        0        0        0   \n",
      "4        1        0        0  ...        1        0        0        0   \n",
      "\n",
      "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0        0        0        0           1           0           0  \n",
      "1        0        0        0           0           0           1  \n",
      "2        0        0        0           0           0           1  \n",
      "3        0        0        0           0           0           1  \n",
      "4        0        0        0           0           0           1  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "------------------------\n",
      "After picking 'Survived', train_y=\n",
      ": 0    0\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    0\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"After dropping 'Survived', train_x=\\n:\",train_x.head())\n",
    "print(\"------------------------\")\n",
    "print(\"After picking 'Survived', train_y=\\n:\",train_y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate the independent and target variable on testing data\n",
    "test_x = test_data.drop(columns=['Survived'],axis=1)\n",
    "test_y = test_data['Survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping 'Survived', test_x=\n",
      ":     Age      Fare  Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  \\\n",
      "0  35.0    7.1250         0         0         1           0         1   \n",
      "1  20.0    7.0500         0         0         1           0         1   \n",
      "2  26.0    7.8958         0         0         1           0         1   \n",
      "3  58.0  146.5208         1         0         0           1         0   \n",
      "4  35.0   83.4750         1         0         0           1         0   \n",
      "\n",
      "   SibSp_0  SibSp_1  SibSp_2  ...  Parch_0  Parch_1  Parch_2  Parch_3  \\\n",
      "0        1        0        0  ...        1        0        0        0   \n",
      "1        1        0        0  ...        1        0        0        0   \n",
      "2        1        0        0  ...        1        0        0        0   \n",
      "3        1        0        0  ...        1        0        0        0   \n",
      "4        0        1        0  ...        1        0        0        0   \n",
      "\n",
      "   Parch_4  Parch_5  Parch_6  Embarked_C  Embarked_Q  Embarked_S  \n",
      "0        0        0        0           0           0           1  \n",
      "1        0        0        0           0           0           1  \n",
      "2        0        0        0           0           0           1  \n",
      "3        0        0        0           1           0           0  \n",
      "4        0        0        0           0           0           1  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "------------------------\n",
      "After picking 'Survived', test_y=\n",
      ": 0    0\n",
      "1    0\n",
      "2    0\n",
      "3    1\n",
      "4    1\n",
      "Name: Survived, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"After dropping 'Survived', test_x=\\n:\",test_x.head())\n",
    "print(\"------------------------\")\n",
    "print(\"After picking 'Survived', test_y=\\n:\",test_y.head())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "Create the object of the Naive Bayes model\n",
    "You can also add other parameters and test your code here\n",
    "Some parameters are : var_smoothing\n",
    "Documentation of sklearn GaussianNB: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
    "\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model with the training data\n",
    "model.fit(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target on train data [1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the train dataset\n",
    "predict_train = model.predict(train_x)\n",
    "print('Target on train data',predict_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score on train dataset :  0.44803370786516855\n"
     ]
    }
   ],
   "source": [
    "# Accuray Score on train dataset\n",
    "accuracy_train = accuracy_score(train_y,predict_train)\n",
    "print('accuracy_score on train dataset : ', accuracy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target on test data [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# predict the target on the test dataset\n",
    "predict_test = model.predict(test_x)\n",
    "print('Target on test data',predict_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score on test dataset :  0.35195530726256985\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score on test dataset\n",
    "accuracy_test = accuracy_score(test_y,predict_test)\n",
    "print('accuracy_score on test dataset : ', accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 Applications of Naive Bayes Algorithms\n",
    "--\n",
    ">1. Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n",
    "\n",
    ">2. Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n",
    "\n",
    ">3. Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n",
    "\n",
    ">4. Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
